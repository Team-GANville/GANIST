{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "",
   "metadata": {
    "tags": [],
    "cell_id": "00002-a3960d36-4e23-4878-95ce-a809883232f0",
    "is_code_hidden": true,
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00002-d48aaa4f-e3c4-4bfd-80c2-fa21d30abf72",
    "is_code_hidden": false,
    "deepnote_to_be_reexecuted": false,
    "source_hash": "8ebf4d8d",
    "execution_start": 1624696167371,
    "execution_millis": 68176515,
    "deepnote_cell_type": "code"
   },
   "source": "# 1. batch_size :  The number of samples to train use in one iteration\n# 2. epochs : The number of times the network goes through the enitre training dataset.\n# 3. Sample_size : [speculation] The number of images[random noise] used in training for generator.\n# 4. nz :[speculation] input feature size for each sample[image] in above sample_size.\n# 5. k : hyper-parameter\n# 6. device : utilizing GPU if available ",
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Dependencies",
   "metadata": {
    "tags": [],
    "cell_id": "00002-84fbcf29-7ceb-40cd-8c10-2131edae520e",
    "deepnote_cell_type": "text-cell-h2"
   }
  },
  {
   "cell_type": "markdown",
   "source": "1. Tqdm :  provides progress bar.\n2. Transform : provides common image transform algorithms.\n3. make_grid : Make a grid of images.\n4. DataLoader: helps in importing training and test sets ",
   "metadata": {
    "tags": [],
    "cell_id": "00003-be2aa102-9fc3-48a7-afee-0c87b9dd8365",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00009-36dedc80-7016-432e-8cab-d284b2ef7735",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "4354564f",
    "execution_start": 1624696167378,
    "execution_millis": 1472,
    "deepnote_cell_type": "code"
   },
   "source": "import torch \nfrom torch import nn\nfrom tqdm.auto import tqdm\nfrom torchvision import transforms\nfrom torchvision.datasets import MNIST\nfrom torchvision.utils import make_grid\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\ntorch.manual_seed(0)",
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 2,
     "data": {
      "text/plain": "<torch._C.Generator at 0x7fb1f129fc50>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "\nTensor:\n\n    1. Tensors are n-dimnesional matrix or vertices.\n    2. Pytorch use tensor operations.\n\nBatches:\n\n    1. Batches are the amount of training samples the model performs operation during a single execution.\n    2. Batch values are usually chosen to be multiples of 2 (32,64,128,256,512).\n\nSumamry :\n    1. https://stackoverflow.com/questions/42480111/model-summary-in-pytorch",
   "metadata": {
    "tags": [],
    "cell_id": "00004-537e6a70-478b-4b63-9e18-56fc3313a624",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "# Dataset\n\n    1. The Dataset is downloaded from the MNIST website . Furtherthe data is divided\n       into batches , converted from PIL to tensors using DataLoaders.\n    2. Dataloader encloses a iterable wrapper over the tensor object using which we\n       can access each batch.\n    3. Structure : 469 batches -> each containg 128 images -> each image  [1,28,28]",
   "metadata": {
    "tags": [],
    "cell_id": "00006-03e85427-fae3-4240-a248-15b664a8257e",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00007-0c5e8c5c-b2ce-4c85-85d0-ffa2d69f7d2b",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "80897178",
    "execution_start": 1624696168854,
    "execution_millis": 0,
    "deepnote_cell_type": "code"
   },
   "source": "loss_func = nn.BCEWithLogitsLoss()\nepochs = 200\nbatch_size = 128\nlr = 0.0001\nz_dim = 64\ndevice = 'cuda'",
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00004-482a4e69-1bab-4f81-a5cc-a1503dc077b0",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "277fa9d0",
    "execution_start": 1624696168863,
    "execution_millis": 39,
    "output_cleared": false,
    "deepnote_cell_type": "code"
   },
   "source": "dataloader = DataLoader( MNIST(\"/work/basicGAN/GAN_cadmus/input\",download = False,transform = transforms.ToTensor() )\n                ,batch_size = batch_size , shuffle = True)",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "text": "/shared-libs/python3.7/py/lib/python3.7/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00009-79bf1256-722d-4406-8a4b-cbde57e66fae",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "fd8ef7e3",
    "execution_start": 1624696168899,
    "execution_millis": 2,
    "deepnote_cell_type": "code"
   },
   "source": "def show_tensor_images(image_tensor,num_images=25,size = (1,28,28)):\n\n    image_unflat = image_tensor.detach().cpu().view(-1, *size)\n    image_grid = make_grid(image_unflat[:num_images], nrow=5)\n    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n    plt.show()",
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### The Generator",
   "metadata": {
    "tags": [],
    "cell_id": "00010-c211dd35-8130-4a19-8e3c-0dc0cf6e023a",
    "deepnote_cell_type": "text-cell-h3"
   }
  },
  {
   "cell_type": "markdown",
   "source": "batch norm - batch normalization lagta hai har ek feature pe ya column pe. Toh usko bas columns chahiye as argument. \nAb gan generator kam hai ki bas layout prepare kare , isse input featurs aur output features set ho jaate hai . Baaki num of sample baadmai pass karne ke time bhejna hota hai. yaha pe 10 feature ke vector ko humne 28*28 pe scale kar diya,",
   "metadata": {
    "tags": [],
    "cell_id": "00011-9165a133-c406-46a1-ba9d-d412b25ee439",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00009-e692a332-f099-49f8-b942-0551dcd2ca9b",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "7e74f704",
    "execution_start": 1624696168925,
    "execution_millis": 1,
    "deepnote_cell_type": "code"
   },
   "source": "def create_gan_layer(input_dim,output_dim):\n    \n    return nn.Sequential(\n\n        nn.Linear(input_dim,output_dim),\n        nn.BatchNorm1d(output_dim),\n        nn.ReLU(inplace = False)\n    )",
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00011-53360c5f-6bc8-4870-aa2a-5564a25755a4",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "2b1c4f89",
    "execution_start": 1624696168926,
    "execution_millis": 0,
    "deepnote_cell_type": "code"
   },
   "source": "class Generator(nn.Module):\n\n    def __init__(self,z_dim,hidden_dim,image_dim):\n\n        super(Generator,self).__init__()\n\n        self.gen = nn.Sequential(\n\n            create_gan_layer(z_dim,hidden_dim),\n            create_gan_layer(hidden_dim,hidden_dim*2),\n            create_gan_layer(hidden_dim*2,hidden_dim*4),\n            create_gan_layer(hidden_dim*4,hidden_dim*8),\n\n            nn.Linear(hidden_dim*8,image_dim),\n            nn.Sigmoid()\n        )\n    \n    def forward(self,noise):\n        return self.gen(noise)",
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Here  the intial linear defintion is just a intialization which creates the structure of the model . This is just for one iamge it ignore the batch_size or all the input samples. \nCalling the model by passing input with batch size to the variable it is assigned to during declaration . here it is gen(noise). Forward acts as the calling function once the object is created.\nhttps://stackoverflow.com/questions/54916135/what-is-the-class-definition-of-nn-linear-in-pytorch",
   "metadata": {
    "tags": [],
    "cell_id": "00012-be1218bc-a575-434a-a377-e193004db49c",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "### Creating&nbsp;Noise&nbsp;Generator",
   "metadata": {
    "tags": [],
    "cell_id": "00014-6c168c92-8089-499e-bc0f-f28155f3fc67",
    "deepnote_cell_type": "text-cell-h3"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00013-a3b4d1d5-3437-4036-801f-9e09168296f8",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "1486c72e",
    "execution_start": 1624696168927,
    "execution_millis": 0,
    "deepnote_cell_type": "code"
   },
   "source": "def noise_generator(num_samples,z_dim,device):\n\n    return torch.randn(num_samples,z_dim,device = device)",
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### The Discriminator",
   "metadata": {
    "tags": [],
    "cell_id": "00015-a7e4e91a-76d3-494c-9c49-83bb9f858f4a",
    "deepnote_cell_type": "text-cell-h3"
   }
  },
  {
   "cell_type": "markdown",
   "source": "1. The create discriminator block is used for creating a layer with leaky relu having slope 0.2.\n2. The Discriminator inherits nn.Module which acts as a base class for all nn Models.\n3. It has three layers with leaky relu as the activation layer , but the last layer which predicts the single dimensional prediction has no activation function because we're using BCELogitloss which is a combination of Sigmoid and BCE loss.\n4. An object is initialized using the constructor arguments  , later the object is passed with the input image which makes an implicit call to the forward function .\n5. Input : 784 features ( pixels ).\n   Hidden Dimensions : (784,512) --> (512,256) --> (256,128)\n   Output : 1 [0/1] (fake/real image).\n",
   "metadata": {
    "tags": [],
    "cell_id": "00018-98b276ec-ed49-4915-92a3-bbceebf78656",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00015-1f5479dc-2f4c-44ca-8205-372b277375fc",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "db057759",
    "execution_start": 1624696168927,
    "execution_millis": 1,
    "deepnote_cell_type": "code"
   },
   "source": "def create_disc_layer(input_dim,output_dim):\n\n    return nn.Sequential(\n\n        nn.Linear(input_dim,output_dim),\n        nn.LeakyReLU(0.2,inplace=True)\n    )",
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00019-a043c902-dbe3-449b-b6c3-b11d8f9099e2",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "4ab08bbf",
    "execution_start": 1624696168931,
    "execution_millis": 5,
    "deepnote_cell_type": "code"
   },
   "source": "class Discriminator(nn.Module):\n\n    def __init__(self,img_dim,hidden_dim,target_dim,device):\n\n        super(Discriminator,self).__init__()\n        \n        self.disc = nn.Sequential(\n\n            create_disc_layer(img_dim,hidden_dim*4),\n            create_disc_layer(hidden_dim*4,hidden_dim*2),\n            create_disc_layer(hidden_dim*2,hidden_dim),\n            \n            nn.Linear(hidden_dim,target_dim)\n        )\n\n    def forward(self,img):\n\n        return self.disc(img) ",
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00020-a15ec934-4193-4dbf-bf2c-1b200ec09eb2",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "dc0e87ca",
    "execution_start": 1624696168941,
    "execution_millis": 6,
    "deepnote_cell_type": "code"
   },
   "source": "discriminator  = Discriminator(784,128,1,\"cuda\")",
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### The Loss Function",
   "metadata": {
    "tags": [],
    "cell_id": "00021-a310157b-4b15-40f3-9dc3-f51285162e94",
    "deepnote_cell_type": "text-cell-h3"
   }
  },
  {
   "cell_type": "markdown",
   "source": "    1. Create a batch of real images , Create another batch of Fake (Generated) Images.\n    2. Pass these to the discriminator object seperately and store them . These are the real and fake pred respectively.\n    3. Get loss for both of the preds for discriminator and generator separately which means there will be four values . (real_disc_loss , fake_disc_loss , real_gen_loss , fake_gen_loss).\n    4 . The loss function used is BCELogitloss which has two arguments (pred,target).\n    5. Pred values are the ones we just obtained , the target values for this model is either a bunch of zeros or 1's based on whether the pred is real or fake.",
   "metadata": {
    "tags": [],
    "cell_id": "00023-5490b8a6-47f7-432f-80ad-2ca8f0a34886",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "![Picture title](image-20210625-115449.png)",
   "metadata": {
    "tags": [],
    "cell_id": "00025-2525f342-1dd6-4ae2-8f8c-f996651b48ed",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "Here the log dx part is the probability of real images being real and 1-logdx is fake being fake. \nHence the classifier (Discriminator needs yo maximizre this) .\n\nWhereas the generator wants the probability of fake images being fake to as low as possible hence it tries minimize the value.\n\n\nAs maximizing we need gradient ascent and as decreasing we need gradient descent . \nIt doesn't use convergence because convergence is the end point we wnat our models to be in equilibrium.",
   "metadata": {
    "tags": [],
    "cell_id": "00026-609c3755-4ae7-44ef-b9b0-97947b4e41a7",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "    create noise\n    generator image from noise , \n    get fake pred, get fake loss , \n    get real images , \n    get real pred , \n    get real loss , \n    disc loss by average\n\n    get noise\n    generate \n    get disc pred\n    get loss but with required target\n",
   "metadata": {
    "tags": [],
    "cell_id": "00026-dc5beadb-854d-4d40-a8c6-e2f1339c01fa",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "source": "real,_ = iter(dataloader).next()\nreal = real.view(real.shape[0],-1)",
   "metadata": {
    "tags": [],
    "cell_id": "00028-1b9c0dc2-72a7-4c40-b7cd-9417273170c6",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "361c4bdb",
    "execution_start": 1624696498598,
    "execution_millis": 0,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00029-d1be61c4-9ef1-4859-b557-c28e1ae4b49d",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "b4b026ac",
    "execution_start": 1624696499124,
    "execution_millis": 20,
    "deepnote_cell_type": "code"
   },
   "source": "bce = nn.BCEWithLogitsLoss()\nz_dim = 10\nhidden_dim = 128\nimg_dim = 784\ndevice = 'cpu'\nbatch_size = real.shape[0]\ngen = Generator( z_dim,hidden_dim,img_dim )\ndisc = Discriminator( img_dim,hidden_dim,1,device)",
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00028-9c5bd967-20d4-48fd-b450-e033d6a68faf",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "52773e84",
    "execution_start": 1624696598869,
    "execution_millis": 1,
    "deepnote_cell_type": "code"
   },
   "source": "def disc_loss(real,num_samples,z_dim,device):\n\n    noise = noise_generator(num_samples,z_dim,device)\n    fake_img = gen(noise)\n    fake_pred = disc(fake_img)\n    fake_loss = bce(fake_pred,torch.zeros_like(fake_pred))\n\n    real  = real\n    real_pred = disc(real)\n    real_loss = bce(real_pred,torch.ones_like(real_pred))\n\n    return ( fake_loss + real_loss ) / 2",
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "disc_loss(real,batch_size,z_dim,device)",
   "metadata": {
    "tags": [],
    "cell_id": "00031-a3260237-4866-4417-880d-fce822a44887",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "5f64f12",
    "execution_start": 1624696613238,
    "execution_millis": 20,
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 31,
     "data": {
      "text/plain": "tensor(0.6902, grad_fn=<DivBackward0>)"
     },
     "metadata": {}
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "source": "def gen_loss(num_samples,z_dim,device):\n\n    noise = noise_generator(num_samples,z_dim,device)\n    fake_img = gen(noise)\n    fake_pred = disc(fake_img)\n    gen_loss = bce(fake_pred,torch.ones_like(fake_pred))\n\n    return gen_loss",
   "metadata": {
    "tags": [],
    "cell_id": "00031-27f56805-d1f7-4596-94d0-3d0c9468c37b",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "1ce0e5c2",
    "execution_start": 1624697173696,
    "execution_millis": 5,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "source": "gen_loss(batch_size,z_dim,device)",
   "metadata": {
    "tags": [],
    "cell_id": "00032-20559642-2c31-483f-b598-9b7fddc1a79a",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "712c2a38",
    "execution_start": 1624697174097,
    "execution_millis": 21,
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 37,
     "data": {
      "text/plain": "tensor(0.6715, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)"
     },
     "metadata": {}
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "source": "### Evaluation",
   "metadata": {
    "tags": [],
    "cell_id": "00025-8e229b79-33a3-43eb-8495-9f516425c860",
    "deepnote_cell_type": "text-cell-h3"
   }
  },
  {
   "cell_type": "markdown",
   "source": "https://stackoverflow.com/questions/49420459/what-is-the-ideal-value-of-loss-function-for-a-gan",
   "metadata": {
    "tags": [],
    "cell_id": "00026-6ff9bff6-29ba-4b4b-a8fe-3a6227ffd414",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=a5eebfae-950c-452c-bcd1-02b41e6a5850' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "b02f753b-6845-4d93-8fa8-18c027434829",
  "deepnote_execution_queue": []
 }
}